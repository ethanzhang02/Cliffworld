{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cliffworld.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKPbLvoEvXTr"
      },
      "source": [
        "# Cliffworld: Dynamic Programming\n",
        "\n",
        "The cliffworld is a 4 by 12 grid, where the bottom row consists of a \"cliff\" along with a start and a goal. We would like our agent to learn to get from start to goal without falling into the cliff. All transitions incur a living reward of -1, except for transitions out of the cliff which incur a cost of -100. The goal state is terminal and has no associated actions.\n",
        "\n",
        "![cliff.png](data:image/png;base64,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)\n",
        "\n",
        "This problem is naturally represented as a MDP with each traversable cell as a state. Actions are left (<), right (>), up (^), and down (v); cliff states ('C') only have the action of teleporting to the start state. These are enumerated by the ```actions``` function . The cardinal direction actions are noisy; the agent moves in its intended direction with probability $p$ and in the two adjacent directions with probability $\\frac12(1-p)$ each. When the agent moves into a wall, it stays in its original state. \n",
        "\n",
        "All of this information is in the ```Qvalue``` function below. A ```state``` is represented as a tuple of two indices that follow the same convention as a Numpy array; top left is (0,0), start is (3,0), and goal is (3,11). Given a ```state```, an ```action```, and a 2D numpy array of ```values```, the function computes the Q-value of taking ```action``` from ```state```, taking into account uncertainties, rewards, and successor state values. A discount factor ```gamma``` may also be applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKExOeBHsBXZ"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def actions(state):\n",
        "  if state == (3,11): return ['G']\n",
        "  if state[0] == 3 and 0 < state[1] < 11: return ['C']\n",
        "  return ['<', '>', '^', 'v']\n",
        "\n",
        "def Qvalue(state, action, values, p, gamma):\n",
        "  \"\"\"\n",
        "  Compute the Q-value for the given state-action pair,\n",
        "  given a set of values for the problem, with successful transition\n",
        "  probability p and discount factor gamma.\n",
        "  \"\"\"\n",
        "  i,j = state\n",
        "  gV = gamma*values\n",
        "  pn = (1-p)/2\n",
        "\n",
        "  # Handle goal and cliff states\n",
        "  if action == 'G':\n",
        "    return 0\n",
        "  if action == 'C':\n",
        "    return -100 + gV[(3,0)]\n",
        "\n",
        "  # All possible successor states\n",
        "  left = (i,max(j-1,0))\n",
        "  right = (i,min(j+1,11))\n",
        "  up = (max(i-1,0),j)\n",
        "  down = (min(i+1,3),j)\n",
        "\n",
        "  # Q-value computation\n",
        "  if action == '<':\n",
        "    return p*(-1+gV[left]) + pn*(-1+gV[up]) + pn*(-1+gV[down])\n",
        "  elif action == '>':\n",
        "    return p*(-1+gV[right]) + pn*(-1+gV[up]) + pn*(-1+gV[down])\n",
        "  elif action == '^':\n",
        "    return p*(-1+gV[up]) + pn*(-1+gV[left]) + pn*(-1+gV[right])\n",
        "  else:\n",
        "    return p*(-1+gV[down]) + pn*(-1+gV[left]) + pn*(-1+gV[right])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rfEAfdTu4n4"
      },
      "source": [
        "def value_iteration(values, p, gamma, threshold=1e-6):\n",
        "  \"\"\"\n",
        "  INPUTS: An initial 2D Numpy array of state values, p and gamma parameters, \n",
        "  and stopping threshold for value iteration\n",
        "  OUTPUTS: Converged 2D Numpy array of state values\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  #array of new values \n",
        "  newvalues = np.zeros_like(values)\n",
        "  maxdiff = float('inf')\n",
        "  while maxdiff>=threshold:\n",
        "    for i in range(len(values)):\n",
        "      for j in range(len(values[i])):\n",
        "        #so now we're in the individual state values\n",
        "        maxqvalue = -float('inf')\n",
        "        state = (i, j)\n",
        "        for action in actions(state):\n",
        "          newqvalue = Qvalue(state, action, values, p, gamma)\n",
        "          if newqvalue > maxqvalue:\n",
        "            maxqvalue = newqvalue\n",
        "        newvalues[i,j] = maxqvalue\n",
        "    diff = np.abs(np.subtract(newvalues, values))\n",
        "    maxdiff = np.amax(diff)\n",
        "    values = np.copy(newvalues)\n",
        "  return values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2An7UtP6u6e_"
      },
      "source": [
        "def extract_policy(values, p, gamma):\n",
        "  \"\"\"\n",
        "  INPUTS: A 2D Numpy array of state values, p and gamma parameters\n",
        "  OUTPUTS: A 2D Numpy array of containing the policy (each cell should contain\n",
        "  an action, one of the following: '<', '>', '^', 'v')\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  #so we want to get the optimal values from value iteration\n",
        "  #check the actions from those states, find best action, save in array \n",
        "  optimalvalues = value_iteration(values, p, gamma)\n",
        "  policies = np.empty_like(values, dtype=object)\n",
        "  for i in range(len(optimalvalues)):\n",
        "    for j in range(len(optimalvalues[i])):\n",
        "      state = (i,j)\n",
        "      maxval = float('-inf')\n",
        "      index = 0\n",
        "      actionslist = actions(state)\n",
        "      argmax = \"\"\n",
        "      for action in actionslist:\n",
        "        qval = Qvalue(state, action, optimalvalues, p, gamma)\n",
        "        if qval > maxval:\n",
        "          maxval = qval\n",
        "          argmax = actionslist[index]\n",
        "        index+=1\n",
        "      policies[i,j] = argmax\n",
        "  return policies\n",
        "\n",
        "def cliffworld_valueiter(p, gamma):\n",
        "  # Find and show the optimal values and policy for the given parameters\n",
        "  values = value_iteration(np.zeros((4,12)), p, gamma)\n",
        "  policy = extract_policy(values, p, gamma) \n",
        "  np.set_printoptions(linewidth=100)\n",
        "  print(np.round(values,2),\"\\n\")\n",
        "  print(policy,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WXPM89Lu8fY"
      },
      "source": [
        "p = 1\n",
        "gamma = 1\n",
        "cliffworld_valueiter(p, gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40CnShOeu_DH"
      },
      "source": [
        "def evaluate_policy(policy, values, p, gamma, threshold=1e-6):\n",
        "  \"\"\"\n",
        "  INPUTS: 2D Numpy arrays containing policy and initial state values, \n",
        "  p and gamma parameters, and stopping threshold for policy evaluation\n",
        "  OUTPUTS: Converged 2D Numpy array of state values\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  newvalues = np.zeros_like(values)\n",
        "  maxdiff = float('inf')\n",
        "  while maxdiff>=threshold:\n",
        "    for i in range(len(values)):\n",
        "      for j in range(len(values[i])):\n",
        "        #so now we're in the individual state values\n",
        "        state = (i, j)\n",
        "        newqvalue = Qvalue(state, policy[i,j], values, p, gamma) \n",
        "        newvalues[i,j] = newqvalue\n",
        "    diff = np.abs(np.subtract(newvalues, values))\n",
        "    maxdiff = np.amax(diff)\n",
        "    values = np.copy(newvalues)\n",
        "  return values\n",
        "\n",
        "\n",
        "def policy_iteration(policy, values, p, gamma):\n",
        "  # Policy iteration wrapper function\n",
        "  while True:\n",
        "    values = evaluate_policy(policy, values, p, gamma)\n",
        "    new_policy = extract_policy(values, p, gamma)\n",
        "    if np.equal(new_policy, policy).all():\n",
        "      return policy, values\n",
        "    policy = new_policy\n",
        "\n",
        "def cliffworld_policyiter(p, gamma):\n",
        "  # Find and show the optimal values and policy for the given parameters\n",
        "  init = extract_policy(np.zeros((4,12)), p, gamma)\n",
        "  policy, values = policy_iteration(init, np.zeros((4,12)), p, gamma)\n",
        "  np.set_printoptions(linewidth=100)\n",
        "  print(np.round(values,2),\"\\n\")\n",
        "  print(policy,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWK9btudvBGt"
      },
      "source": [
        "p = 1\n",
        "gamma = 0.99\n",
        "cliffworld_policyiter(p, gamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsSfrQkDvLEl"
      },
      "source": [
        "# Cliffworld: Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9dqPiFAvChH"
      },
      "source": [
        "import random\n",
        "\n",
        "def epsilon_greedy_action(Qvalues, state, epsilon):\n",
        "  # Explore a random action from state with probability epsilon\n",
        "  # Otherwise, greedily choose the best action\n",
        "  if random.random() < epsilon:\n",
        "    return random.choice(actions(state))\n",
        "  else:\n",
        "    Qvalue = -float(\"inf\")\n",
        "    for a in actions(state):\n",
        "      if Qvalues[(state, a)] > Qvalue:\n",
        "        action = a\n",
        "        Qvalue = Qvalues[(state, a)]\n",
        "    return action  \n",
        "\n",
        "def step(state, action, p):\n",
        "  # Return successor state and reward upon taking action from state\n",
        "  i,j = state\n",
        "  if action == 'C':\n",
        "    return (3,0), -100\n",
        "\n",
        "  if action == '<':\n",
        "    if random.random() < p: return (i,max(j-1,0)), -1\n",
        "    else: return random.choice([(max(i-1,0),j), (min(i+1,3),j)]), -1\n",
        "  if action == '>':\n",
        "    if random.random() < p: return (i,min(j+1,11)), -1\n",
        "    else: return random.choice([(max(i-1,0),j), (min(i+1,3),j)]), -1\n",
        "  if action == '^':\n",
        "    if random.random() < p: return (max(i-1,0),j), -1\n",
        "    else: return random.choice([(i,max(j-1,0)), (i,min(j+1,11))]), -1\n",
        "  else:\n",
        "    if random.random() < p: return (min(i+1,3),j), -1\n",
        "    else: return random.choice([(i,max(j-1,0)), (i,min(j+1,11))]), -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJcMcwIcvDxk"
      },
      "source": [
        "def SARSA(Qvalues, p, gamma, alpha, epsilon, episodes=50000):\n",
        "  # SARSA temporal difference learning using initial Qvalues and given parameters\n",
        "  # Returns a learned policy (Numpy 2d array)\n",
        "  for i in range(episodes):\n",
        "    state = (3,0)\n",
        "    action = epsilon_greedy_action(Qvalues, state, epsilon)\n",
        "    while state != (3,11):\n",
        "      next_state, reward = step(state, action, p)\n",
        "      next_action = epsilon_greedy_action(Qvalues, next_state, epsilon)\n",
        "      target = Qvalues[(next_state, next_action)]\n",
        "      Qvalues[(state, action)] += alpha * (reward + gamma*target - Qvalues[(state, action)])\n",
        "      state = next_state\n",
        "      action = next_action\n",
        "  policy = extract_policy(Qvalues)\n",
        "  return policy\n",
        "\n",
        "def extract_policy(Qvalues):\n",
        "  # Extract the optimal policy associated with the given Q-values\n",
        "  policy = np.empty((4,12), dtype=object)\n",
        "  for i in range(4):\n",
        "    for j in range(12):\n",
        "      policy[i,j] = epsilon_greedy_action(Qvalues, (i,j), 0)\n",
        "  return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcGEOao3vFC1"
      },
      "source": [
        "def Qlearner(Qvalues, p, gamma, alpha, epsilon, episodes=50000):\n",
        "  # Q-learning using initial Qvalues and given parameters\n",
        "  # Returns a learned policy (2D Numpy array)  \n",
        "  # YOUR CODE HERE\n",
        "  for i in range(episodes):\n",
        "    state = (3,0)\n",
        "    action = epsilon_greedy_action(Qvalues, state, epsilon)\n",
        "    while state != (3,11):\n",
        "      next_state, reward = step(state, action, p)\n",
        "      next_action = epsilon_greedy_action(Qvalues, next_state, epsilon)\n",
        "      best_action = epsilon_greedy_action(Qvalues, next_state, 0)\n",
        "      target = Qvalues[(next_state, best_action)]\n",
        "      Qvalues[(state, action)] += alpha * (reward + gamma*target - Qvalues[(state, action)])\n",
        "      state = next_state\n",
        "      action = next_action\n",
        "  policy = extract_policy(Qvalues)\n",
        "  return policy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz8QJ5WdvGYX"
      },
      "source": [
        "def TD_learn(p, gamma, alpha, epsilon):\n",
        "  Qvalues = {((i,j),a): 0 for i in range(4) for j in range(12) for a in actions((i,j))}\n",
        "  policy = SARSA(Qvalues, p, gamma, alpha, epsilon)\n",
        "  print(\"SARSA policy\")   \n",
        "  print(policy,\"\\n\")\n",
        "  \n",
        "  Qvalues = {((i,j),a): 0 for i in range(4) for j in range(12) for a in actions((i,j))}\n",
        "  policy = Qlearner(Qvalues, p, gamma, alpha, epsilon)\n",
        "  print(\"Q-learning policy\")  \n",
        "  print(policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRuquufCvHmm"
      },
      "source": [
        "p = 1\n",
        "gamma = 1\n",
        "alpha = 0.1\n",
        "epsilon = 0.2\n",
        "TD_learn(p, gamma, alpha, epsilon)\n",
        "\n",
        "p = 0.8\n",
        "gamma = 1\n",
        "alpha = 0.1\n",
        "epsilon = 0.2\n",
        "TD_learn(p, gamma, alpha, epsilon)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}